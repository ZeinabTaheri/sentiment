{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZeinabTaheri_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl6qeM9gUhLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#در این قسمت ابتدا کتابخانه های مورد نیاز را نصب میکنیم\n",
        "!pip install emoji\n",
        "!pip install --upgrade tensorflow==2.0.0\n",
        "!pip install --upgrade tensorflow-gpu==2.0.0\n",
        "!pip install https://github.com/sobhe/hazm/archive/master.zip --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwg0LeNVUxZb",
        "colab_type": "code",
        "outputId": "fe37936d-a7b7-4373-a2e1-9c6783f2192b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# در این قسمت کتابخانه های لازم را ایمپورت میکنیم\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoiboLALrBO",
        "colab_type": "text"
      },
      "source": [
        "# section 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrBvXRNRUE3i",
        "colab_type": "code",
        "outputId": "e08bba27-3e77-4379-8c0f-361aa077b5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# آدرس فایل جملات\n",
        "sentence_data = 'drive/My Drive/Untitled folder/fa_2.xlsx'\n",
        "\n",
        "# خواندن فایل ورودی و جدا کردن جملات و لیبل آن ها به صورت دو لیست متفاوت\n",
        "data = pd.read_excel(sentence_data)\n",
        "sentences = list(data.text)\n",
        "labels = list(data.label)\n",
        "\n",
        "# ذخیره جملات هر کلاس در یک لیست جداگانه\n",
        "neg_sentences = []\n",
        "pos_sentences = []\n",
        "med_sentences = []\n",
        "\n",
        "for sen, l in zip(sentences, labels):\n",
        "  if l == 'neg':\n",
        "    neg_sentences.append(sen)\n",
        "  if l == 'pos':\n",
        "    pos_sentences.append(sen)\n",
        "  if l == 'med':\n",
        "    med_sentences.append(sen)\n",
        "\n",
        "print(\"We have %d neg sentences\"%len(neg_sentences))\n",
        "print(\"We have %d pos sentences\"%len(pos_sentences))\n",
        "print(\"We have %d med sentences\"%len(med_sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 3070 neg sentences\n",
            "We have 1619 pos sentences\n",
            "We have 311 med sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9kwLaKCXHpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# نوشتن یک نرمالایزر برای نرمال کردن دیتا با استفاده از هضم \n",
        "def Normalizer_text(input_text, normalizer):\n",
        "\n",
        "  # حذف فاصله های اضافی در متن\n",
        "  input_text = input_text.rstrip('\\r\\n').strip()\n",
        "  #حذف نام کاربری از متن\n",
        "  normalized_text = re.sub('@[^\\s]+','', input_text)\n",
        "  # حذف آدرس های اینترنتی از متن\n",
        "  normalized_text = re.sub(r\"http\\S+\", \"\", normalized_text)\n",
        "  # نرمال کردن متن با هضم\n",
        "  normalized_text = normalizer.normalize(normalized_text)\n",
        "  # حذف برخی از علامت های نگارشی\n",
        "  normalized_text = normalized_text.replace('«', ' ').replace('»', ' ')\\\n",
        "    .replace('\"', ' ').replace('#', ' ').replace('-', ' ').replace('_', ' ')\\\n",
        "    .replace('*', ' ').replace('…', ' ').replace(\"'\", ' ').replace('\\n\\n', ' ')\\\n",
        "    .replace('\\n', ' ').replace('^', ' ')\n",
        "  # توکن کردن دیتا\n",
        "  tokenized_text = word_tokenize(normalized_text)\n",
        "  # حذف حروفی که بیشتر از دوبار پشت سر هم تکرار شده اند.\n",
        "  token_list = [re.sub(r'(.)\\1+', r'\\1\\1', token) for token in tokenized_text]\n",
        "\n",
        "  return token_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srgq0tsZVGIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "\n",
        "# تقسیم دیتا به دو بخش آموزشی و تست\n",
        "for sen in neg_sentences[:2870]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('neg')\n",
        "\n",
        "for sen in neg_sentences[2870:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('neg')\n",
        "\n",
        "for sen in pos_sentences[:1540]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('pos')\n",
        "\n",
        "for sen in pos_sentences[1540:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('pos')\n",
        "\n",
        "for sen in med_sentences[:280]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('med')\n",
        "\n",
        "for sen in med_sentences[280:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('med')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7QW8o0zV94T",
        "colab_type": "code",
        "outputId": "20b73ad1-918f-4e4f-ea1f-f0b2f94d8c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# این تابع برای تبدیل کلمات جملات به ایندکس نوشته شده است\n",
        "def make_numberized_samples():\n",
        "  my_tokens=[]\n",
        "  for sen in X_train:\n",
        "    for token in sen:\n",
        "      my_tokens.append(token)\n",
        "\n",
        "  print('We have {} tokens.'.format(len(my_tokens)))\n",
        "  my_tokens = set(my_tokens)\n",
        "  print('We have {} unique tokens.'.format(len(my_tokens)))\n",
        "\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  word2idx['UNK'] = 1\n",
        "  idx2word[1] = 'UNK'\n",
        "  for i, token in enumerate(my_tokens):\n",
        "    word2idx[token] = i+2\n",
        "    idx2word[i+2] = token\n",
        "\n",
        "  label2idx = {}\n",
        "  idx2label = {}\n",
        "  label2idx['neg'] = 0\n",
        "  label2idx['pos'] = 1\n",
        "  label2idx['med'] = 2\n",
        "  idx2label[0] = 'neg'\n",
        "  idx2label[1] = 'pos'\n",
        "  idx2label[2] = 'med'\n",
        "\n",
        "  numberized_sen_train=[]\n",
        "  numberized_labels_train=[]\n",
        "\n",
        "  numberized_sen_test=[]\n",
        "  numberized_labels_test=[]\n",
        "\n",
        "  for sen in X_train:\n",
        "    x=[]\n",
        "    for t in sen:\n",
        "      x.append(word2idx[t])\n",
        "    numberized_sen_train.append(x)\n",
        "\n",
        "  for l in y_train:\n",
        "    numberized_labels_train.append(label2idx[l])\n",
        "\n",
        "  for sen in X_test:\n",
        "    x=[]\n",
        "    for t in sen:\n",
        "      try:\n",
        "        x.append(word2idx[t])\n",
        "      except:\n",
        "        x.append(1)\n",
        "    numberized_sen_test.append(x)\n",
        "\n",
        "  for l in y_test:\n",
        "    numberized_labels_test.append(label2idx[l])\n",
        "  \n",
        "  numberized_sen_train=np.asarray(numberized_sen_train)\n",
        "  numberized_labels_train=np.asarray(numberized_labels_train)\n",
        "  numberized_sen_test=np.asarray(numberized_sen_test)\n",
        "  numberized_labels_test=np.asarray(numberized_labels_test)\n",
        "\n",
        "  indices = np.arange(numberized_sen_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  np.random.shuffle(indices)\n",
        "  numberized_sen_train = numberized_sen_train[indices]\n",
        "  numberized_labels_train = numberized_labels_train[indices]\n",
        "\n",
        "  indices = np.arange(numberized_sen_test.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  np.random.shuffle(indices)\n",
        "  numberized_sen_test = numberized_sen_test[indices]\n",
        "  numberized_labels_test = numberized_labels_test[indices]\n",
        "\n",
        "\n",
        "  pickle_data = [numberized_sen_train, numberized_labels_train,\\\n",
        "                numberized_sen_test, numberized_labels_test,\\\n",
        "                    word2idx, idx2word, label2idx, idx2label]\n",
        "\n",
        "  pickle_address = 'drive/My Drive/Untitled folder/data_11.pkl'\n",
        "  with open(pickle_address, 'wb') as f:\n",
        "    pkl.dump(pickle_data, f)\n",
        "\n",
        "  print('Saved as pickle file')\n",
        "\n",
        "make_numberized_samples()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 123114 tokens.\n",
            "We have 20812 unique tokens.\n",
            "Saved as pickle file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6LLlgyocFkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# فایل ذخیره شده در تابع قبل را میخوانیم\n",
        "pickle_address = 'drive/My Drive/Untitled folder/data_1.pkl'\n",
        "with open(pickle_address, 'rb') as myData:\n",
        "  numberized_sen_train, numberized_labels_train,\\\n",
        "    numberized_sen_test, numberized_labels_test,\\\n",
        "      word2idx, idx2word, label2idx, idx2label = pkl.load(myData)\n",
        "max_len = max(len(sentences) for sentences in numberized_sen_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY11P3PVcYH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# جملات ورودی در شبکه باید طول یکسانی داشته باشند. با این تابع طول همه جملات را یکسان میکنیم\n",
        "numberized_sen_train = pad_sequences(numberized_sen_train, maxlen = max_len, padding='post')\n",
        "numberized_sen_test = pad_sequences(numberized_sen_test, maxlen = max_len, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dikbv9FudF50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "n_epochs = 10\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM6wom9kdKw8",
        "colab_type": "code",
        "outputId": "858cacf5-e38d-42d9-93b6-9b806305f3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# تعریف لایه امبدینگ برای تبدیل کلمات به بردار 300 بعدی\n",
        "embedding_layer = Embedding(len(word2idx)+1,\\\n",
        "                            EMBEDDING_DIM,\\\n",
        "                            input_length=max_len,\\\n",
        "                            trainable=True)\n",
        "\n",
        "# تعریف لایه ورودی \n",
        "sequence_input = Input(shape=(max_len, ), dtype=tf.int32)\n",
        "\n",
        "# ورودی را با استفاده از لایه امبدینگ به بردار تبدیل میکنیم\n",
        "embedded_sequence = embedding_layer(sequence_input)\n",
        "\n",
        "# تعریف لایه RNN\n",
        "# تابع فعالیت relu - تعداد نورون 128\n",
        "bilstm_1 = Bidirectional(SimpleRNN(units=128, activation='relu',\\\n",
        "                              return_sequences=True))(embedded_sequence)\n",
        "# استفاده از dropout ->> جلوگیری از بیش براز\n",
        "bilstm_1 = Dropout(0.3)(bilstm_1)\n",
        "\n",
        "# تعریف لایه RNN\n",
        "# تابع فعالیت relu - تعداد نورون 128\n",
        "bilstm_2 = Bidirectional(SimpleRNN(units=128, activation='relu',\\\n",
        "                              return_sequences=False))(bilstm_1)                              \n",
        "# استفاده از dropout ->> جلوگیری از بیش براز\n",
        "bilstm_2 = Dropout(0.3)(bilstm_2)\n",
        "\n",
        "# تعریف لایه پرسپترون برای استخراج ویژگی های بهتر\n",
        "dense_1 = Dense(units=64, activation='relu')(bilstm_2)\n",
        "# استفاده از dropout ->> جلوگیری از بیش براز\n",
        "dense_1 = Dropout(0.3)(dense_1)\n",
        "# لایه خروجی با تابع فعالیت softmax\n",
        "pred = Dense(units=len(label2idx), activation='softmax')(dense_1)\n",
        "\n",
        "# تعریف ورودی و مشخص کردن ورودی و خروجی\n",
        "model = Model(inputs=[sequence_input], outputs=pred)\n",
        "# کامپایل کردن مدل و تعریف تابع خطا و بهینه ساز\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics= ['accuracy'])\n",
        "\n",
        "# گرفتن سامری از مدل\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_15 (InputLayer)        [(None, 99)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 99, 300)           6244200   \n",
            "_________________________________________________________________\n",
            "bidirectional_16 (Bidirectio (None, 99, 256)           109824    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 99, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_17 (Bidirectio (None, 256)               98560     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 6,469,227\n",
            "Trainable params: 6,469,227\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcKeXEyYedEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# شروع آموزش\n",
        "history = model.fit(numberized_sen_train, numberized_labels_train,\\\n",
        "                    validation_data=(numberized_sen_test, numberized_labels_test),\\\n",
        "                     epochs=n_epochs, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoSO8EH19rAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ذخیره مدل آموزش دیده شده \n",
        "model.save('drive/My Drive/Untitled folder/1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rAZTau4Yi0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# لود کردن مدل\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('drive/My Drive/Untitled folder/1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4qJ_pBY4Dm",
        "colab_type": "code",
        "outputId": "af5bd1c1-2ce7-4b3e-d3eb-f40c96940600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# ارزیابی مدل\n",
        "loss, acc = model.evaluate(numberized_sen_test, numberized_labels_test)\n",
        "print('loss: {}, accuracy: {}'.format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r310/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.7577 - accuracy: 0.7258\n",
            "loss: 1.8788646382670249, accuracy: 0.725806474685669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq38txOFILI1",
        "colab_type": "text"
      },
      "source": [
        "# section 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yt8k5ANLwcd",
        "colab_type": "code",
        "outputId": "246f8b96-62ad-474c-b9a8-f9cb6daf046b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# آدرس فایل جملات\n",
        "sentence_data = 'drive/My Drive/Untitled folder/fa_2.xlsx'\n",
        "\n",
        "# خواندن فایل ورودی و جدا کردن جملات و لیبل آن ها به صورت دو لیست متفاوت\n",
        "data = pd.read_excel(sentence_data)\n",
        "sentences = list(data.text)\n",
        "labels = list(data.label)\n",
        "\n",
        "# ذخیره جملات هر کلاس در یک لیست جداگانه\n",
        "neg_sentences = []\n",
        "pos_sentences = []\n",
        "med_sentences = []\n",
        "\n",
        "for sen, l in zip(sentences, labels):\n",
        "  if l == 'neg':\n",
        "    neg_sentences.append(sen)\n",
        "  if l == 'pos':\n",
        "    pos_sentences.append(sen)\n",
        "  if l == 'med':\n",
        "    med_sentences.append(sen)\n",
        "\n",
        "print(\"We have %d neg sentences\"%len(neg_sentences))\n",
        "print(\"We have %d pos sentences\"%len(pos_sentences))\n",
        "print(\"We have %d med sentences\"%len(med_sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 3070 neg sentences\n",
            "We have 1619 pos sentences\n",
            "We have 311 med sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt_2TMk7L0FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# نوشتن یک نرمالایزر برای نرمال کردن دیتا با استفاده از هضم \n",
        "def Normalizer_text(input_text, normalizer):\n",
        "\n",
        "  # حذف فاصله های اضافی در متن\n",
        "  input_text = input_text.rstrip('\\r\\n').strip()\n",
        "  #حذف نام کاربری از متن\n",
        "  normalized_text = re.sub('@[^\\s]+','', input_text)\n",
        "  # حذف آدرس های اینترنتی از متن\n",
        "  normalized_text = re.sub(r\"http\\S+\", \"\", normalized_text)\n",
        "  # نرمال کردن متن با هضم\n",
        "  normalized_text = normalizer.normalize(normalized_text)\n",
        "  # حذف برخی از علامت های نگارشی\n",
        "  normalized_text = normalized_text.replace('«', ' ').replace('»', ' ')\\\n",
        "    .replace('\"', ' ').replace('#', ' ').replace('-', ' ').replace('_', ' ')\\\n",
        "    .replace('*', ' ').replace('…', ' ').replace(\"'\", ' ').replace('\\n\\n', ' ')\\\n",
        "    .replace('\\n', ' ').replace('^', ' ')\n",
        "  # توکن کردن دیتا\n",
        "  tokenized_text = word_tokenize(normalized_text)\n",
        "  # حذف حروفی که بیشتر از دوبار پشت سر هم تکرار شده اند.\n",
        "  token_list = [re.sub(r'(.)\\1+', r'\\1\\1', token) for token in tokenized_text]\n",
        "\n",
        "  return token_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg7x3NG5L3ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "\n",
        "# تقسیم دیتا به دو بخش آموزشی و تست\n",
        "for sen in neg_sentences[:2870]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('neg')\n",
        "\n",
        "for sen in neg_sentences[2870:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('neg')\n",
        "\n",
        "for sen in pos_sentences[:1540]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('pos')\n",
        "\n",
        "for sen in pos_sentences[1540:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('pos')\n",
        "\n",
        "for sen in med_sentences[:280]:\n",
        "  X_train.append(Normalizer_text(sen, normalizer))\n",
        "  y_train.append('med')\n",
        "\n",
        "for sen in med_sentences[280:]:\n",
        "  X_test.append(Normalizer_text(sen, normalizer))\n",
        "  y_test.append('med')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siN9SjogINeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# خواندن فایل ویژگی های هر کلمه\n",
        "word_data = 'drive/My Drive/Untitled folder/Phrases.xlsx'\n",
        "\n",
        "# خواندن فایل و ذخیره کلمات و ویژگی ها به صورت لیست\n",
        "data = pd.read_excel(word_data)\n",
        "words = list(data.PersianTranslation)\n",
        "pos = list(data.Positive)\n",
        "neg = list(data.Negative)\n",
        "anger = list(data.Anger)\n",
        "anticipation = list(data.Anticipation)\n",
        "disgust = list(data.Disgust)\n",
        "fear = list(data.Fear)\n",
        "joy = list(data.Joy)\n",
        "sadness = list(data.Sadness)\n",
        "surprise = list(data.Surprise)\n",
        "trust = list(data.Trust)\n",
        "\n",
        "# تعریف یک دیکشنری و ذخیره هر کلمه به همراه ویژگی آن در دیکشنری\n",
        "words_features = {}\n",
        "\n",
        "for w, p, n, ang, ant, d, f, j, sad, sur, t in zip(words, pos, neg, anger, anticipation, disgust, fear, joy, sadness, surprise, trust):\n",
        "  words_features[w] = [p, n, ang, ant, d, f, j, sad, sur, t]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEhKAVPPL9G2",
        "colab_type": "code",
        "outputId": "50d6d2dc-92ec-4994-ae92-3dfd6a3311a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# تبدیل کلمات به بردار عددی در این تابع انجام میشود\n",
        "def make_numberized_samples():\n",
        "  my_tokens=[]\n",
        "  for sen in X_train:\n",
        "    for token in sen:\n",
        "      my_tokens.append(token)\n",
        "\n",
        "  print('We have {} tokens.'.format(len(my_tokens)))\n",
        "  my_tokens = set(my_tokens)\n",
        "  print('We have {} unique tokens.'.format(len(my_tokens)))\n",
        "\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  word2idx['UNK'] = 1\n",
        "  idx2word[1] = 'UNK'\n",
        "  for i, token in enumerate(my_tokens):\n",
        "    word2idx[token] = i+2\n",
        "    idx2word[i+2] = token\n",
        "\n",
        "  label2idx = {}\n",
        "  idx2label = {}\n",
        "  label2idx['neg'] = 0\n",
        "  label2idx['pos'] = 1\n",
        "  label2idx['med'] = 2\n",
        "  idx2label[0] = 'neg'\n",
        "  idx2label[1] = 'pos'\n",
        "  idx2label[2] = 'med'\n",
        "\n",
        "  numberized_sen_train=[]\n",
        "  numberized_features_train = []\n",
        "  numberized_labels_train=[]\n",
        "\n",
        "  numberized_sen_test=[]\n",
        "  numberized_features_test = []\n",
        "  numberized_labels_test=[]\n",
        "\n",
        "  for sen in X_train:\n",
        "    x=[]\n",
        "    for t in sen:\n",
        "      x.append(word2idx[t])\n",
        "    numberized_sen_train.append(x)\n",
        "\n",
        "  for sen in X_train:\n",
        "    f=[]\n",
        "    for t in sen:\n",
        "      try:\n",
        "        f.append(np.asarray(words_features[t]))\n",
        "      except:\n",
        "        f.append(np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
        "    numberized_features_train.append(f)\n",
        "\n",
        "  for l in y_train:\n",
        "    numberized_labels_train.append(label2idx[l])\n",
        "\n",
        "  for sen in X_test:\n",
        "    x=[]\n",
        "    for t in sen:\n",
        "      try:\n",
        "        x.append(word2idx[t])\n",
        "      except:\n",
        "        x.append(1)\n",
        "    numberized_sen_test.append(x)\n",
        "\n",
        "  for sen in X_test:\n",
        "    f=[]\n",
        "    for t in sen:\n",
        "      try:\n",
        "        f.append(np.asarray(words_features[t]))\n",
        "      except:\n",
        "        f.append(np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
        "    numberized_features_test.append(f)\n",
        "\n",
        "  for l in y_test:\n",
        "    numberized_labels_test.append(label2idx[l])\n",
        "  \n",
        "  numberized_sen_train=np.asarray(numberized_sen_train)\n",
        "  numberized_features_train = np.asarray(numberized_features_train)\n",
        "  numberized_labels_train=np.asarray(numberized_labels_train)\n",
        "  numberized_sen_test=np.asarray(numberized_sen_test)\n",
        "  numberized_features_test = np.asarray(numberized_features_test)\n",
        "  numberized_labels_test=np.asarray(numberized_labels_test)\n",
        "\n",
        "  indices = np.arange(numberized_sen_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  np.random.shuffle(indices)\n",
        "  numberized_sen_train = numberized_sen_train[indices]\n",
        "  numberized_features_train = numberized_features_train[indices]\n",
        "  numberized_labels_train = numberized_labels_train[indices]\n",
        "\n",
        "  indices = np.arange(numberized_sen_test.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  np.random.shuffle(indices)\n",
        "  numberized_sen_test = numberized_sen_test[indices]\n",
        "  numberized_features_test = numberized_features_test[indices]\n",
        "  numberized_labels_test = numberized_labels_test[indices]\n",
        "\n",
        "\n",
        "  pickle_data = [numberized_sen_train, numberized_features_train, numberized_labels_train,\\\n",
        "                 numberized_sen_test, numberized_features_test, numberized_labels_test,\\\n",
        "                    word2idx, idx2word, label2idx, idx2label]\n",
        "\n",
        "  pickle_address = 'drive/My Drive/Untitled folder/data_2.pkl'\n",
        "  with open(pickle_address, 'wb') as f:\n",
        "    pkl.dump(pickle_data, f)\n",
        "\n",
        "  print('Saved as pickle file')\n",
        "\n",
        "make_numberized_samples()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 123114 tokens.\n",
            "We have 20812 unique tokens.\n",
            "Saved as pickle file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6_bd8UtN3RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# خواندن خروجی های ذخیره شده از تابع قبل\n",
        "pickle_address = 'drive/My Drive/Untitled folder/data_2.pkl'\n",
        "with open(pickle_address, 'rb') as myData:\n",
        "  numberized_sen_train, numberized_features_train, numberized_labels_train,\\\n",
        "    numberized_sen_test, numberized_features_test, numberized_labels_test,\\\n",
        "      word2idx, idx2word, label2idx, idx2label = pkl.load(myData)\n",
        "max_len = max(len(sentences) for sentences in numberized_sen_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfMGP7CPvAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# جملات ورودی در شبکه باید طول یکسانی داشته باشند. با این تابع طول همه جملات را یکسان میکنیم\n",
        "numberized_sen_train = pad_sequences(numberized_sen_train, maxlen = max_len, padding='post')\n",
        "numberized_features_train = pad_sequences(numberized_features_train, maxlen = max_len, padding='post')\n",
        "\n",
        "numberized_sen_test = pad_sequences(numberized_sen_test, maxlen = max_len, padding='post')\n",
        "numberized_features_test = pad_sequences(numberized_features_test, maxlen = max_len, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w1NBeM9bWTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "n_epochs = 10\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TilPPiOXbdCk",
        "colab_type": "code",
        "outputId": "901a2b37-857c-4dc9-b4c6-3515f7d5ab22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# تعریف لایه امبدینگ برای تبدیل کلمات به بردار 300 بعدی\n",
        "embedding_layer = Embedding(len(word2idx)+1,\\\n",
        "                            EMBEDDING_DIM,\\\n",
        "                            input_length=max_len,\\\n",
        "                            trainable=True)\n",
        "\n",
        "# تعریف لایه های  ورودی \n",
        "\n",
        "# ورودی خود کلمات\n",
        "sequence_input = Input(shape=(max_len, ), dtype=tf.int32)\n",
        "\n",
        "# ورودی ویژگی های اضافه شده\n",
        "feature_input = Input(shape=(max_len, 10), dtype=tf.float32)\n",
        "\n",
        "# تبدیل کلمات به بردار ویژگی توسط لایه امبدینگ\n",
        "embedded_sequence = embedding_layer(sequence_input)\n",
        "\n",
        "# ترکیب بردار کلمات بدست آمده از امبدینگ و بردار ویژگی هر کلمه\n",
        "concat = concatenate([embedded_sequence, feature_input], axis=2)\n",
        "\n",
        "# تعریف لایه RNN\n",
        "# تابع فعالیت relu - تعداد نورون 128\n",
        "bilstm_1 = Bidirectional(SimpleRNN(units=128, activation='relu',\\\n",
        "                              return_sequences=True))(concat)\n",
        "\n",
        "# استفاده از dropout ->> جلوگیری از بیش براز                              \n",
        "bilstm_1 = Dropout(0.35)(bilstm_1)\n",
        "\n",
        "# تعریف لایه RNN\n",
        "# تابع فعالیت relu - تعداد نورون 128\n",
        "bilstm_2 = Bidirectional(SimpleRNN(units=128, activation='relu',\\\n",
        "                              return_sequences=False))(bilstm_1)                              \n",
        "\n",
        "# استفاده از dropout ->> جلوگیری از بیش براز                                 \n",
        "bilstm_2 = Dropout(0.35)(bilstm_2)\n",
        "\n",
        "# استفاده از یک لایه پرسپترون برای استخراج ویژگی مناسب تر\n",
        "dense_1 = Dense(units=64, activation='relu')(bilstm_2)\n",
        "# استفاده از dropout ->> جلوگیری از بیش براز   \n",
        "dense_1 = Dropout(0.35)(dense_1)\n",
        "\n",
        "# لایه خروجی با تابع فعالیت softmax\n",
        "pred = Dense(units=len(label2idx), activation='softmax')(dense_1)\n",
        "\n",
        "# تعریف ورودی و مشخص کردن ورودی و خروجی\n",
        "model = Model(inputs=[sequence_input, feature_input], outputs=pred)\n",
        "\n",
        "# کامپایل کردن مدل و تعریف تابع خطا و بهینه ساز\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 99, 10)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 99)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 99, 300)      6244200     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 99, 10)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 99, 310)      0           embedding_2[0][0]                \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 99, 256)      112384      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 99, 256)      0           bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 256)          98560       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 256)          0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           16448       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 3)            195         dropout_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,471,787\n",
            "Trainable params: 6,471,787\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NvHr5Ucbhly",
        "colab_type": "code",
        "outputId": "2b20e400-5e42-4152-fb8e-4b426466f222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history = model.fit([numberized_sen_train, numberized_features_train], numberized_labels_train,\\\n",
        "                      validation_data=([numberized_sen_test, numberized_features_test], numberized_labels_test),\\\n",
        "                        epochs=n_epochs, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4690 samples, validate on 310 samples\n",
            "Epoch 1/10\n",
            "4690/4690 [==============================] - 44s 9ms/sample - loss: 0.8605 - accuracy: 0.6006 - val_loss: 0.8657 - val_accuracy: 0.6516\n",
            "Epoch 2/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.6846 - accuracy: 0.7139 - val_loss: 0.6801 - val_accuracy: 0.7516\n",
            "Epoch 3/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.3200 - accuracy: 0.8906 - val_loss: 0.7715 - val_accuracy: 0.7452\n",
            "Epoch 4/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.1697 - accuracy: 0.9356 - val_loss: 0.9240 - val_accuracy: 0.7710\n",
            "Epoch 5/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.1031 - accuracy: 0.9676 - val_loss: 1.3531 - val_accuracy: 0.6903\n",
            "Epoch 6/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.0727 - accuracy: 0.9800 - val_loss: 1.4107 - val_accuracy: 0.6645\n",
            "Epoch 7/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.0476 - accuracy: 0.9868 - val_loss: 1.5676 - val_accuracy: 0.7097\n",
            "Epoch 8/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.0220 - accuracy: 0.9947 - val_loss: 2.0854 - val_accuracy: 0.7258\n",
            "Epoch 9/10\n",
            "4690/4690 [==============================] - 39s 8ms/sample - loss: 0.0110 - accuracy: 0.9981 - val_loss: 2.1369 - val_accuracy: 0.7484\n",
            "Epoch 10/10\n",
            "4690/4690 [==============================] - 38s 8ms/sample - loss: 0.0080 - accuracy: 0.9985 - val_loss: 1.9253 - val_accuracy: 0.7355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXx_emhuLJnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ذخیره مدل\n",
        "model.save('drive/My Drive/Untitled folder/2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KDuBLa8PcoKx",
        "colab": {}
      },
      "source": [
        "# لود کردن مدل\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('drive/My Drive/Untitled folder/2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "589be83f-35d9-4f1c-d025-2f10560366fb",
        "id": "fJ11UMjwcoK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# ارزیابی مدل\n",
        "loss, acc = model.evaluate([numberized_sen_test, numberized_features_test], numberized_labels_test)\n",
        "print('loss: {}, accuracy: {}'.format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r310/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2388 - accuracy: 0.7355\n",
            "loss: 1.9253126129027336, accuracy: 0.7354838848114014\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
